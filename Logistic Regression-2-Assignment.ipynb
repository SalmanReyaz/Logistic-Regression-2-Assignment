{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158e5929",
   "metadata": {},
   "source": [
    "# Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4051913",
   "metadata": {},
   "source": [
    "Grid Search with Cross-Validation (Grid Search CV) is a hyperparameter tuning technique commonly used in machine learning to systematically search for the best combination of hyperparameters for a model. Its primary purpose is to automate the process of hyperparameter optimization to find the set of hyperparameters that results in the best model performance.\n",
    "\n",
    "Here's how Grid Search CV works:\n",
    "\n",
    "1. Hyperparameters:\n",
    "   - Machine learning models have hyperparameters that are not learned from the data but need to be set before training the model. Examples of hyperparameters include the learning rate in a neural network, the maximum depth of a decision tree, or the regularization strength in a logistic regression model.\n",
    "   - These hyperparameters significantly influence a model's performance, and selecting appropriate values can be crucial for achieving good results.\n",
    "\n",
    "2. Grid Search:\n",
    "   - Grid Search CV involves specifying a grid of hyperparameter values to explore. For each hyperparameter, we define a range of potential values or a set of discrete choices.\n",
    "   - The grid is essentially a Cartesian product of all possible combinations of hyperparameters. For example, if we have two hyperparameters, each with three possible values, we would have a 3x3 grid with nine combinations.\n",
    "\n",
    "3. Cross-Validation:\n",
    "   - To evaluate the performance of each hyperparameter combination, Grid Search CV uses cross-validation. Cross-validation involves splitting the dataset into multiple subsets (folds), using some of them for training and others for validation.\n",
    "   - The typical choice is k-fold cross-validation, where the dataset is divided into k equally-sized folds, and the model is trained and evaluated k times, each time using a different fold for validation and the remaining folds for training.\n",
    "\n",
    "4. Model Training and Evaluation:\n",
    "   - For each combination of hyperparameters in the grid, Grid Search CV trains a model using the training data for each fold and evaluates it on the validation data.\n",
    "   - It computes a performance metric (e.g., accuracy, F1-score, or mean squared error) for each combination based on the results of cross-validation.\n",
    "\n",
    "5. Hyperparameter Tuning:\n",
    "   - Grid Search CV identifies the hyperparameter combination that resulted in the best performance on the validation data across all cross-validation folds.\n",
    "   - This best combination is selected as the optimal set of hyperparameters for the model.\n",
    "\n",
    "6. Final Model Training:\n",
    "   - Once the optimal hyperparameters are determined, the model is trained on the entire dataset (or a training set) using these hyperparameters to create the final model.\n",
    "\n",
    "7. Model Evaluation:\n",
    "   - The final model can be evaluated on a separate test dataset to estimate its performance on unseen data.\n",
    "\n",
    "`The purpose of Grid Search CV is to find the best hyperparameters without the need for manual and time-consuming experimentation. It systematically explores the hyperparameter space and ensures that the model's performance is optimized. It is a crucial step in the machine learning pipeline to fine-tune models and achieve better predictive accuracy and generalization.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817dcdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "884d4928",
   "metadata": {},
   "source": [
    "# Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25174273",
   "metadata": {},
   "source": [
    "`GridSearchCV and RandomizedSearchCV are both powerful tools for hyperparameter tuning in machine learning. They both systematically explore different combinations of hyperparameter values and evaluate their performance using cross-validation. However, they differ in their approach to exploring the hyperparameter space.`\n",
    "\n",
    "`GridSearchCV performs an exhaustive search, evaluating all possible combinations of hyperparameter values within the specified grid. This ensures that the optimal set of hyperparameters is found, but it can be computationally expensive, especially for a large number of hyperparameters or a complex model.`\n",
    "`\n",
    "`RandomizedSearchCV, on the other hand, performs a random search, evaluating a randomly sampled subset of the possible combinations of hyperparameter values. This makes it more efficient than GridSearchCV, but it may not always find the optimal set of hyperparameters.`\n",
    "\n",
    "## When to choose GridSearchCV:\n",
    "\n",
    "`When the number of hyperparameters is relatively small`\n",
    "\n",
    "\n",
    "`When the model is not too computationally expensive to train`\n",
    "\n",
    "\n",
    "`When we need to guarantee that we have found the optimal set of hyperparameters`\n",
    "\n",
    "\n",
    "## When to choose RandomizedSearchCV:\n",
    "\n",
    "`When the number of hyperparameters is large`\n",
    "\n",
    "`When the model is computationally expensive to train`\n",
    "\n",
    "`When we are more interested in finding a good set of hyperparameters rather than the absolute best set`\n",
    "\n",
    "\n",
    "`In general, RandomizedSearchCV is a good starting point for hyperparameter tuning, especially when dealing with a large number of hyperparameters or a complex model. If we have the computational resources, we can then fine-tune the hyperparameters using GridSearchCV.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a3a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40b12e53",
   "metadata": {},
   "source": [
    "# Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded483a4",
   "metadata": {},
   "source": [
    "\n",
    "`Data leakage is a situation in machine learning where training data is shared with the model in a way that violates the intended training process. This can happen in various ways, such as:`\n",
    "\n",
    "`Using features from the future in the training data: This can lead to the model memorizing the future data, which will make it unable to generalize to new data.`\n",
    "\n",
    "\n",
    "`Using features that are not available for new data: For example, if the training data includes user IDs, but the model will only be used on new data for which the user IDs are unknown, then using the user IDs in the training data will cause the model to overfit to the training data.`\n",
    "\n",
    "`Using features that are correlated with the target variable: This can cause the model to learn the relationship between the features and the target variable from the training data, even though the relationship may not be true in general.`\n",
    "\n",
    "`Data leakage can have several negative consequences, including:`\n",
    "\n",
    "`Poor generalization: The model will not be able to make accurate predictions on new data, since it has learned to rely on the leaked data.`\n",
    "`\n",
    "`Bias: The model will be biased towards the specific data that it has seen in the training data, which may not be representative of the real world`.\n",
    "\n",
    "`Reduced interpretability: The model will be more difficult to interpret, since it will be learning from patterns in the leaked data that are not directly related to the target variable.`\n",
    "\n",
    "## Here is an example of data leakage:\n",
    "\n",
    "## A company is developing a model to predict whether a customer will churn (stop using their service). The company has data on all of their customers, including whether they churned or not. They also have data on the customers' usage of their service, such as how many times they logged in, how many times they made purchases, and how much money they spent.\n",
    "\n",
    "### The company decides to use this usage data to train their model. However, they do not realize that this data is leaked, since it contains information about the future (whether the customer churned or not). As a result, the model learns to predict churn based on the usage data, which will not be helpful for making predictions about new customers.\n",
    "\n",
    "`To avoid data leakage, it is important to carefully design the training process and to carefully review the data that is being used. It is also important to use techniques such as data anonymization and data partitioning to prevent the model from learning about information that is not relevant to the target variable.`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafa41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f82c119",
   "metadata": {},
   "source": [
    "# Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad2bcb33",
   "metadata": {},
   "source": [
    "Data leakage is a significant issue in machine learning that can lead to inaccurate predictions and biased results. It occurs when information from the training data is inadvertently included in the evaluation or prediction process, causing the model to learn patterns that are not present in the real world. To prevent data leakage and ensure the reliability of wer machine learning models, follow these guidelines:\n",
    "\n",
    "Separate training and evaluation data:\n",
    "Create distinct datasets for training and evaluation. The training data is used to build the model, while the evaluation data is used to assess its performance on unseen data. Avoid using any information from the evaluation data during the training process.\n",
    "\n",
    "Careful feature selection:\n",
    "Choose features that are relevant to the target variable and are available for both training and evaluation. Avoid using features that are only available during the training process or are correlated with the target variable in a way that is not generalizable to real-world scenarios.\n",
    "\n",
    "Data preprocessing consistency:\n",
    "Apply the same data preprocessing steps to both training and evaluation data. This ensures that the model is trained and evaluated on data that is consistent in terms of scaling, normalization, and feature engineering.\n",
    "\n",
    "Use cross-validation:\n",
    "Employ cross-validation techniques to evaluate the model's performance on different partitions of the training data. This helps identify potential data leakage issues and prevents overfitting.\n",
    "\n",
    "Regularization:\n",
    "Apply regularization techniques, such as L1 or L2 regularization, to penalize large coefficients in the model. This can help reduce the model's sensitivity to noise and reduce the impact of data leakage.\n",
    "\n",
    "Data anonymization:\n",
    "If the training data contains sensitive or confidential information, consider anonymizing it before using it to train the model. This can help protect privacy and reduce the risk of data leakage.\n",
    "\n",
    "Data partitioning:\n",
    "Divide the training data into smaller partitions and train multiple models on different partitions. Evaluate the performance of each model on a held-out partition to assess the model's generalizability and identify potential data leakage issues.\n",
    "\n",
    "Careful monitoring and review:\n",
    "Monitor the model's performance on new data over time and be vigilant for any signs of data leakage or performance degradation. Regularly review the data sources and preprocessing steps to ensure the continued integrity of the model's training data.\n",
    "\n",
    "Seek expert advice:\n",
    "If you are unsure about potential data leakage issues or need guidance in implementing data leakage prevention strategies, consider consulting with experienced machine learning practitioners or data scientists. They can provide valuable insights and help we ensure the reliability of wer machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cdba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9721bfbe",
   "metadata": {},
   "source": [
    "# Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b59d72",
   "metadata": {},
   "source": [
    "## A confusion matrix is a table used in classification to evaluate the performance of a machine learning model. It summarizes the results of the model's predictions on a set of data, showing the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.\n",
    "\n",
    "## `Here's a breakdown of the terms in a confusion matrix:`\n",
    "\n",
    "`- True Positive (TP): Instances where the model correctly predicts the positive class.`\n",
    "\n",
    "`- True Negative (TN): Instances where the model correctly predicts the negative class.`\n",
    "\n",
    "`- False Positive (FP): Instances where the model incorrectly predicts the positive class (Type I error).`\n",
    "\n",
    "`- False Negative (FN): Instances where the model incorrectly predicts the negative class (Type II error).`\n",
    "\n",
    "\n",
    "## `The confusion matrix provides a more detailed understanding of a classifier's performance than simple accuracy. From these values, several performance metrics can be derived:`\n",
    "\n",
    "`1. Accuracy: The proportion of correctly classified instances out of the total instances. It's calculated as (TP + TN) / (TP + TN + FP + FN).`\n",
    "\n",
    "`2. Precision (Positive Predictive Value): The proportion of true positive predictions out of the total predicted positives. It's calculated as TP / (TP + FP).`\n",
    "\n",
    "`3. Recall (Sensitivity, True Positive Rate): The proportion of true positive predictions out of the total actual positives. It's calculated as TP / (TP + FN).`\n",
    "\n",
    "`4. Specificity (True Negative Rate): The proportion of true negative predictions out of the total actual negatives. It's calculated as TN / (TN + FP).`\n",
    "\n",
    "`5. F1 Score: The harmonic mean of precision and recall. It provides a balance between precision and recall and is calculated as 2  (Precision  Recall) / (Precision + Recall).`\n",
    "\n",
    "`By examining these metrics, you can gain insights into different aspects of your model's performance. For example, high precision indicates a low rate of false positives, while high recall indicates a low rate of false negatives. The choice of which metric to prioritize depends on the specific goals and requirements of your application.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f840db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3717f5f7",
   "metadata": {},
   "source": [
    "# Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121cf03",
   "metadata": {},
   "source": [
    "## Precision and recall are two important metrics derived from a confusion matrix, and they provide insights into different aspects of a classification model's performance.\n",
    "\n",
    "`1. Precision:`\n",
    "   - Formula: Precision = TP / (TP + FP)\n",
    "   - Precision focuses on the accuracy of the positive predictions made by the model.\n",
    "   - It answers the question: Of all the instances predicted as positive, how many were actually positive?\n",
    "   - High precision indicates that the model has a low rate of false positives (instances wrongly predicted as positive).\n",
    "\n",
    "`2. Recall (Sensitivity or True Positive Rate):`\n",
    "   - Formula: Recall = TP / (TP + FN)\n",
    "   - Recall focuses on the ability of the model to capture all the positive instances in the dataset.\n",
    "   - It answers the question: Of all the actual positive instances, how many were correctly predicted by the model?\n",
    "   - High recall indicates that the model has a low rate of false negatives (instances wrongly predicted as negative).\n",
    "\n",
    "`In summary:`\n",
    "- Precision is concerned with the accuracy of positive predictions, emphasizing the avoidance of false positives.\n",
    "- Recall is concerned with the ability to capture all positive instances, emphasizing the avoidance of false negatives.\n",
    "\n",
    "The balance between precision and recall depends on the specific goals of the application. In some cases, such as medical diagnosis, high recall might be more important to ensure that all relevant cases are captured, even if it means more false positives. In other cases, such as spam detection, high precision might be more critical to minimize false alarms, even if it means missing some spam emails (lower recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b282e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6d93761",
   "metadata": {},
   "source": [
    "# Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc60926",
   "metadata": {},
   "source": [
    "Interpreting a confusion matrix allows you to understand the types of errors your model is making and assess its performance. Let's break down how to analyze a confusion matrix:\n",
    "\n",
    "Consider the following confusion matrix:\n",
    "\n",
    "```\n",
    "                 Actual Class 1   Actual Class 0\n",
    "Predicted Class 1        TP               FP\n",
    "Predicted Class 0        FN               TN\n",
    "```\n",
    "\n",
    "1. True Positive (TP): The model correctly predicted instances of Class 1.\n",
    "   - Interpretation: These are the instances where the model got it right, predicting positive when the actual class was positive.\n",
    "\n",
    "2. True Negative (TN): The model correctly predicted instances of Class 0.\n",
    "   - Interpretation: These are instances where the model got it right, predicting negative when the actual class was negative.\n",
    "\n",
    "3. False Positive (FP): The model incorrectly predicted instances of Class 1.\n",
    "   - Interpretation: These are instances where the model made a positive prediction, but the actual class was negative. It's a Type I error.\n",
    "\n",
    "4. False Negative (FN): The model incorrectly predicted instances of Class 0.\n",
    "   - Interpretation: These are instances where the model made a negative prediction, but the actual class was positive. It's a Type II error.\n",
    "\n",
    "Analyzing these values helps you understand the specific errors your model is making:\n",
    "\n",
    "- Type I Error (False Positive): The model predicts positive when it shouldn't.\n",
    "  - Implications: This can lead to unnecessary actions or resources being allocated when they're not needed.\n",
    "\n",
    "- Type II Error (False Negative): The model predicts negative when it should have predicted positive.\n",
    "  - Implications: This can result in missing important instances or opportunities.\n",
    "\n",
    "By looking at the distribution of TP, TN, FP, and FN, you can calculate metrics such as precision, recall, accuracy, and F1 score to get a comprehensive understanding of your model's strengths and weaknesses. Adjusting the model or its thresholds may be necessary based on these insights to improve overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705a4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e08c6be2",
   "metadata": {},
   "source": [
    "# Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f172dbcd",
   "metadata": {},
   "source": [
    "Several common metrics can be derived from a confusion matrix, providing insights into different aspects of a classification model's performance. Here are some of the key metrics and their formulas:\n",
    "\n",
    "1. Accuracy:\n",
    "   - Formula: Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "   - Measures the overall correctness of the model's predictions.\n",
    "\n",
    "2. Precision (Positive Predictive Value):\n",
    "   - Formula: Precision = TP / (TP + FP)\n",
    "   - Focuses on the accuracy of positive predictions, emphasizing the avoidance of false positives.\n",
    "\n",
    "3. Recall (Sensitivity or True Positive Rate):\n",
    "   - Formula: Recall = TP / (TP + FN)\n",
    "   - Focuses on the ability of the model to capture all positive instances, emphasizing the avoidance of false negatives.\n",
    "\n",
    "4. Specificity (True Negative Rate):\n",
    "   - Formula: Specificity = TN / (TN + FP)\n",
    "   - Measures the ability of the model to correctly identify negative instances.\n",
    "\n",
    "5. F1 Score:\n",
    "   - Formula: F1 Score = 2  (Precision  Recall) / (Precision + Recall)\n",
    "   - The harmonic mean of precision and recall, providing a balance between the two.\n",
    "\n",
    "6. False Positive Rate (FPR):\n",
    "   - Formula: FPR = FP / (FP + TN)\n",
    "   - Measures the proportion of actual negatives that were incorrectly predicted as positive.\n",
    "\n",
    "7. False Negative Rate (FNR):\n",
    "   - Formula: FNR = FN / (FN + TP)\n",
    "   - Measures the proportion of actual positives that were incorrectly predicted as negative.\n",
    "\n",
    "8. Matthews Correlation Coefficient (MCC):\n",
    "   - Formula: MCC = (TP  TN - FP  FN) / sqrt((TP + FP)  (TP + FN)  (TN + FP)  (TN + FN))\n",
    "   - Takes into account all four values in the confusion matrix and provides a balanced measure of classification performance.\n",
    "\n",
    "These metrics offer a comprehensive view of a model's performance, considering trade-offs between different types of errors. The choice of which metric to prioritize depends on the specific goals and requirements of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2c7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "618f3666",
   "metadata": {},
   "source": [
    "# Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632465f",
   "metadata": {},
   "source": [
    "`The accuracy of a model is calculated as the percentage of correct predictions that the model makes. It can be calculated from the confusion matrix using the following formula:`\n",
    "\n",
    "`Accuracy = (True Positives + True Negatives) / Total Predictions`\n",
    "\n",
    "\n",
    "`The higher the accuracy, the better the model is at performing the classification task. However, it is important to note that accuracy can be misleading if the dataset is imbalanced, meaning that there are many more instances of one class than the other. For example, if a dataset contains 99% negative instances and 1% positive instances, a model that simply predicts that all instances are negative will have an accuracy of 99%. However, this model is not very useful, as it is not able to identify the positive instances.`\n",
    "\n",
    "`The confusion matrix can also be used to calculate other performance metrics, such as precision, recall, and F1-score. These metrics can be more informative than accuracy in some cases. For example, if the cost of a false positive is high, then precision is a more important metric than accuracy.`\n",
    "\n",
    "`Overall, the relationship between the accuracy of a model and the values in its confusion matrix is as follows:`\n",
    "\n",
    "`A higher accuracy indicates that the model is making more correct predictions.\n",
    "However, accuracy can be misleading if the dataset is imbalanced.\n",
    "Other performance metrics, such as precision, recall, and F1-score, can be more informative than accuracy in some cases.\n",
    "It is important to consider the specific context of the classification task when choosing which performance metrics to use to evaluate the model.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54417ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54ad3957",
   "metadata": {},
   "source": [
    "# Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268b069",
   "metadata": {},
   "source": [
    "A confusion matrix can be a valuable tool for identifying potential biases or limitations in a machine learning model, particularly when it comes to understanding how the model performs across different classes or groups. Here's how we can use a confusion matrix for this purpose:\n",
    "\n",
    "1. Class Imbalance:\n",
    "   - Check if there's a significant imbalance in the distribution of classes. If one class vastly outnumbers the others, the model might be biased towards predicting the majority class. Look for a disproportionate number of false positives or false negatives in the minority class.\n",
    "\n",
    "2. False Positives and False Negatives:\n",
    "   - Examine the false positives and false negatives in each class. Identify whether the model is more prone to making certain types of errors. This can reveal biases in the model's predictions.\n",
    "\n",
    "3. Precision and Recall Disparities:\n",
    "   - Compare precision and recall across different classes. A large disparity between precision and recall for a particular class may indicate a bias or limitation. For example, a high recall but low precision might suggest that the model is making a large number of false positive predictions for that class.\n",
    "\n",
    "4. Group-specific Performance:\n",
    "   - If wer dataset includes different groups or demographics, analyze the model's performance within each group. Look for variations in accuracy, precision, and recall. Significant differences may indicate biases in how well the model generalizes to different subgroups.\n",
    "\n",
    "5. Sensitivity to Input Features:\n",
    "   - Investigate whether the model's performance varies based on specific input features. Biases may arise if the model relies heavily on certain features and struggles with others, especially if those features are correlated with sensitive attributes.\n",
    "\n",
    "6. Fairness Metrics:\n",
    "   - Utilize fairness metrics to quantitatively assess disparities in predictions across different groups. Fairness metrics like disparate impact, equalized odds, and demographic parity can help identify and measure biases in model predictions.\n",
    "\n",
    "7. Confusion Matrix Visualization:\n",
    "   - Create visualizations of the confusion matrix, such as heatmaps or stacked bar charts, to easily spot patterns and imbalances. Visualization can provide a quick and intuitive understanding of where the model may be falling short.\n",
    "\n",
    "By thoroughly analyzing the confusion matrix and related metrics, we can uncover potential biases and limitations in wer machine learning model, allowing we to address and mitigate these issues to improve overall fairness and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df8d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
